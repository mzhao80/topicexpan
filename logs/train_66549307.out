No checkpoint found. Starting training from scratch.
Tokenizing documents and phrases using bert-base-uncased tokenizer, please wait...
TopicExpan(
  (doc_encoder): BertDocEncoder(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (phrase_decoder): TransformerPhraseDecoder(
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output_embeddings): Linear(in_features=768, out_features=30522, bias=False)
    (model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (topic_encoder): GCNTopicEncoder(
    (downward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=300, out=300, normalization=right, activation=None)
    )
    (upward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=300, out=300, normalization=right, activation=None)
    )
    (sideward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=300, out=300, normalization=right, activation=None)
    )
  )
  (interaction): BilinearInteraction()
  (linear_combiner): Linear(in_features=1068, out_features=768, bias=True)
)
Trainable parameters: 142394216
Train Epoch: 1 [0/7437 (0%)] Loss: 305.981262 [39.848404 + 266.132843]
Train Epoch: 1 [1480/7437 (20%)] Loss: 160.657806 [17.421789 + 143.236023]
Train Epoch: 1 [2960/7437 (40%)] Loss: 131.860321 [16.864334 + 114.995995]
Train Epoch: 1 [4440/7437 (60%)] Loss: 201.562408 [16.156380 + 185.406036]
Train Epoch: 1 [5920/7437 (80%)] Loss: 97.060440 [17.582273 + 79.478165]
Start validation epoch: 1
Validation Epoch: 1 [0/921 (0%)] Loss: 107.023254
    epoch          : 1
    elapsed time   : 1092.3503227233887
    loss           : 149.48292033291796
    sim_loss       : 17.986566406378397
    gen_loss       : 131.4963539488023
    val_loss       : 118.51173553466796
    val_sim_loss   : 15.307031297683716
    val_gen_loss   : 103.20470390319824
    val_perplexity : 15.209503173828125
    val_accuracy   : 0.0
Saving checkpoint: congress-save/models/checkpoint-epoch1.pth ...
Saving current best: model_best.pth ...
Train Epoch: 2 [0/7437 (0%)] Loss: 151.157776 [16.474537 + 134.683243]
Train Epoch: 2 [1480/7437 (20%)] Loss: 113.278549 [16.922647 + 96.355904]
Train Epoch: 2 [2960/7437 (40%)] Loss: 102.902023 [16.635376 + 86.266647]
Train Epoch: 2 [4440/7437 (60%)] Loss: 101.587814 [16.065956 + 85.521858]
Train Epoch: 2 [5920/7437 (80%)] Loss: 72.492737 [17.068947 + 55.423786]
Start validation epoch: 2
Validation Epoch: 2 [0/921 (0%)] Loss: 158.284149
    epoch          : 2
    elapsed time   : 1092.1195414066315
    loss           : 110.07810388869494
    sim_loss       : 16.691193085151696
    gen_loss       : 93.38691074400332
    val_loss       : 105.8184326171875
    val_sim_loss   : 15.217833721637726
    val_gen_loss   : 90.6005989074707
    val_perplexity : 11.75241756439209
    val_accuracy   : 0.05
Saving checkpoint: congress-save/models/checkpoint-epoch2.pth ...
Saving current best: model_best.pth ...
Train Epoch: 3 [0/7437 (0%)] Loss: 133.628189 [15.820927 + 117.807266]
Train Epoch: 3 [1480/7437 (20%)] Loss: 94.407684 [16.225834 + 78.181854]
