2025-01-16 10:55:43,736 - train - INFO - TopicExpan(
  (doc_encoder): BertDocEncoder(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (phrase_decoder): TransformerPhraseDecoder(
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output_embeddings): Linear(in_features=768, out_features=30522, bias=False)
    (model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (topic_encoder): GCNTopicEncoder(
    (downward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (upward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (sideward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
  )
  (interaction): BilinearInteraction()
  (linear_combiner): Linear(in_features=1152, out_features=768, bias=True)
)
Trainable parameters: 142868480
2025-01-16 10:55:46,891 - trainer - INFO - ================================================================================
2025-01-16 10:55:46,891 - trainer - INFO - Starting epoch 1 at 2025-01-16 10:55:46
2025-01-16 11:15:24,612 - trainer - INFO - [2025-01-16 11:15:24] Starting validation for epoch: 1
2025-01-16 11:15:39,918 - trainer - INFO - Epoch 1 completed at 2025-01-16 11:15:39
2025-01-16 11:15:39,918 - trainer - INFO -     epoch          : 1
2025-01-16 11:15:39,918 - trainer - INFO -     elapsed time   : 1193.0260972976685
2025-01-16 11:15:39,918 - trainer - INFO -     loss           : 222.4671649762054
2025-01-16 11:15:39,918 - trainer - INFO -     sim_loss       : 16.62437293446479
2025-01-16 11:15:39,918 - trainer - INFO -     gen_loss       : 205.84279196819855
2025-01-16 11:15:39,918 - trainer - INFO -     val_loss       : 186.2902294505726
2025-01-16 11:15:39,918 - trainer - INFO -     val_sim_loss   : 15.123241424560547
2025-01-16 11:15:39,918 - trainer - INFO -     val_gen_loss   : 171.16698559847745
2025-01-16 11:15:39,918 - trainer - INFO -     val_perplexity : 23.03707504272461
2025-01-16 11:15:39,918 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 11:15:39,918 - trainer - INFO - ================================================================================
2025-01-16 11:15:39,918 - trainer - INFO - Starting epoch 2 at 2025-01-16 11:15:39
2025-01-16 11:49:40,185 - train - INFO - TopicExpan(
  (doc_encoder): BertDocEncoder(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (phrase_decoder): TransformerPhraseDecoder(
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output_embeddings): Linear(in_features=768, out_features=30522, bias=False)
    (model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (topic_encoder): GCNTopicEncoder(
    (downward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (upward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (sideward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
  )
  (interaction): BilinearInteraction()
  (linear_combiner): Linear(in_features=1152, out_features=768, bias=True)
)
Trainable parameters: 142868480
2025-01-16 11:49:42,921 - trainer - INFO - ================================================================================
2025-01-16 11:49:42,924 - trainer - INFO - Starting epoch 1 at 2025-01-16 11:49:42
2025-01-16 12:09:31,728 - trainer - INFO - [2025-01-16 12:09:31] Starting validation for epoch: 1
2025-01-16 12:09:46,948 - trainer - INFO - Epoch 1 completed at 2025-01-16 12:09:46
2025-01-16 12:09:46,948 - trainer - INFO -     epoch          : 1
2025-01-16 12:09:46,948 - trainer - INFO -     elapsed time   : 1204.0234820842743
2025-01-16 12:09:46,948 - trainer - INFO -     loss           : 221.869243731902
2025-01-16 12:09:46,948 - trainer - INFO -     sim_loss       : 16.023930433377696
2025-01-16 12:09:46,948 - trainer - INFO -     gen_loss       : 205.84531352484404
2025-01-16 12:09:46,948 - trainer - INFO -     val_loss       : 184.43622450395063
2025-01-16 12:09:46,948 - trainer - INFO -     val_sim_loss   : 13.281745303760875
2025-01-16 12:09:46,948 - trainer - INFO -     val_gen_loss   : 171.15448067405006
2025-01-16 12:09:46,948 - trainer - INFO -     val_perplexity : 23.036277770996094
2025-01-16 12:09:46,948 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 12:09:46,948 - trainer - INFO - ================================================================================
2025-01-16 12:09:46,948 - trainer - INFO - Starting epoch 2 at 2025-01-16 12:09:46
2025-01-16 12:29:34,245 - trainer - INFO - [2025-01-16 12:29:34] Starting validation for epoch: 2
2025-01-16 12:29:49,227 - trainer - INFO - Epoch 2 completed at 2025-01-16 12:29:49
2025-01-16 12:29:49,228 - trainer - INFO -     epoch          : 2
2025-01-16 12:29:49,228 - trainer - INFO -     elapsed time   : 1202.278757572174
2025-01-16 12:29:49,228 - trainer - INFO -     loss           : 182.6425155715563
2025-01-16 12:29:49,228 - trainer - INFO -     sim_loss       : 11.682683713044693
2025-01-16 12:29:49,228 - trainer - INFO -     gen_loss       : 170.95983179955934
2025-01-16 12:29:49,228 - trainer - INFO -     val_loss       : 176.77055220170453
2025-01-16 12:29:49,228 - trainer - INFO -     val_sim_loss   : 8.137305823239414
2025-01-16 12:29:49,228 - trainer - INFO -     val_gen_loss   : 168.6332452947443
2025-01-16 12:29:49,228 - trainer - INFO -     val_perplexity : 23.410362243652344
2025-01-16 12:29:49,228 - trainer - INFO -     val_accuracy   : 0.022727272727272728
2025-01-16 12:29:49,228 - trainer - INFO - ================================================================================
2025-01-16 12:29:49,228 - trainer - INFO - Starting epoch 3 at 2025-01-16 12:29:49
2025-01-16 12:49:37,347 - trainer - INFO - [2025-01-16 12:49:37] Starting validation for epoch: 3
2025-01-16 12:49:52,394 - trainer - INFO - Epoch 3 completed at 2025-01-16 12:49:52
2025-01-16 12:49:52,394 - trainer - INFO -     epoch          : 3
2025-01-16 12:49:52,394 - trainer - INFO -     elapsed time   : 1203.165292263031
2025-01-16 12:49:52,394 - trainer - INFO -     loss           : 162.90985290755086
2025-01-16 12:49:52,394 - trainer - INFO -     sim_loss       : 8.453137877924526
2025-01-16 12:49:52,394 - trainer - INFO -     gen_loss       : 154.45671523364624
2025-01-16 12:49:52,394 - trainer - INFO -     val_loss       : 175.84430729259145
2025-01-16 12:49:52,394 - trainer - INFO -     val_sim_loss   : 7.206202983856201
2025-01-16 12:49:52,394 - trainer - INFO -     val_gen_loss   : 168.63810140436345
2025-01-16 12:49:52,394 - trainer - INFO -     val_perplexity : 22.315271377563477
2025-01-16 12:49:52,394 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 12:49:52,394 - trainer - INFO - ================================================================================
2025-01-16 12:49:52,394 - trainer - INFO - Starting epoch 4 at 2025-01-16 12:49:52
2025-01-16 13:09:38,938 - trainer - INFO - [2025-01-16 13:09:38] Starting validation for epoch: 4
2025-01-16 13:09:54,024 - trainer - INFO - Epoch 4 completed at 2025-01-16 13:09:54
2025-01-16 13:09:54,024 - trainer - INFO -     epoch          : 4
2025-01-16 13:09:54,024 - trainer - INFO -     elapsed time   : 1201.6294600963593
2025-01-16 13:09:54,024 - trainer - INFO -     loss           : 148.60965466855177
2025-01-16 13:09:54,024 - trainer - INFO -     sim_loss       : 7.757846985232844
2025-01-16 13:09:54,024 - trainer - INFO -     gen_loss       : 140.85180781729778
2025-01-16 13:09:54,024 - trainer - INFO -     val_loss       : 176.1847431009466
2025-01-16 13:09:54,024 - trainer - INFO -     val_sim_loss   : 5.812467835166237
2025-01-16 13:09:54,024 - trainer - INFO -     val_gen_loss   : 170.37227223136207
2025-01-16 13:09:54,025 - trainer - INFO -     val_perplexity : 21.94144630432129
2025-01-16 13:09:54,025 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 13:09:54,025 - trainer - INFO - ================================================================================
2025-01-16 13:09:54,025 - trainer - INFO - Starting epoch 5 at 2025-01-16 13:09:54
2025-01-16 13:29:42,455 - trainer - INFO - [2025-01-16 13:29:42] Starting validation for epoch: 5
2025-01-16 13:29:57,628 - trainer - INFO - Epoch 5 completed at 2025-01-16 13:29:57
2025-01-16 13:29:57,628 - trainer - INFO -     epoch          : 5
2025-01-16 13:29:57,628 - trainer - INFO -     elapsed time   : 1203.6032733917236
2025-01-16 13:29:57,629 - trainer - INFO -     loss           : 135.59005999209276
2025-01-16 13:29:57,629 - trainer - INFO -     sim_loss       : 6.78221147036671
2025-01-16 13:29:57,629 - trainer - INFO -     gen_loss       : 128.80784855553168
2025-01-16 13:29:57,629 - trainer - INFO -     val_loss       : 178.1423225402832
2025-01-16 13:29:57,629 - trainer - INFO -     val_sim_loss   : 6.122251207178289
2025-01-16 13:29:57,629 - trainer - INFO -     val_gen_loss   : 172.02007189663973
2025-01-16 13:29:57,629 - trainer - INFO -     val_perplexity : 24.733304977416992
2025-01-16 13:29:57,629 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 13:30:14,152 - trainer - INFO - Saving checkpoint: congress-save/models/checkpoint-epoch5.pth ...
2025-01-16 13:30:14,153 - trainer - INFO - ================================================================================
2025-01-16 13:30:14,153 - trainer - INFO - Starting epoch 6 at 2025-01-16 13:30:14
2025-01-16 13:50:02,779 - trainer - INFO - [2025-01-16 13:50:02] Starting validation for epoch: 6
2025-01-16 13:50:17,929 - trainer - INFO - Epoch 6 completed at 2025-01-16 13:50:17
2025-01-16 13:50:17,929 - trainer - INFO -     epoch          : 6
2025-01-16 13:50:17,929 - trainer - INFO -     elapsed time   : 1203.7759203910828
2025-01-16 13:50:17,929 - trainer - INFO -     loss           : 124.61204950512938
2025-01-16 13:50:17,929 - trainer - INFO -     sim_loss       : 6.439581373319104
2025-01-16 13:50:17,930 - trainer - INFO -     gen_loss       : 118.17246818732266
2025-01-16 13:50:17,930 - trainer - INFO -     val_loss       : 178.8780299100009
2025-01-16 13:50:17,930 - trainer - INFO -     val_sim_loss   : 5.807640184055675
2025-01-16 13:50:17,930 - trainer - INFO -     val_gen_loss   : 173.0703905278986
2025-01-16 13:50:17,930 - trainer - INFO -     val_perplexity : 22.953216552734375
2025-01-16 13:50:17,930 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 13:50:17,930 - trainer - INFO - ================================================================================
2025-01-16 13:50:17,930 - trainer - INFO - Starting epoch 7 at 2025-01-16 13:50:17
2025-01-16 14:10:07,175 - trainer - INFO - [2025-01-16 14:10:07] Starting validation for epoch: 7
2025-01-16 14:10:22,398 - trainer - INFO - Epoch 7 completed at 2025-01-16 14:10:22
2025-01-16 14:10:22,398 - trainer - INFO -     epoch          : 7
2025-01-16 14:10:22,398 - trainer - INFO -     elapsed time   : 1204.4681220054626
2025-01-16 14:10:22,399 - trainer - INFO -     loss           : 114.93448512139013
2025-01-16 14:10:22,399 - trainer - INFO -     sim_loss       : 6.295642060705401
2025-01-16 14:10:22,399 - trainer - INFO -     gen_loss       : 108.6388431606008
2025-01-16 14:10:22,399 - trainer - INFO -     val_loss       : 181.84891908819026
2025-01-16 14:10:22,399 - trainer - INFO -     val_sim_loss   : 5.9482619979164815
2025-01-16 14:10:22,399 - trainer - INFO -     val_gen_loss   : 175.90065748041326
2025-01-16 14:10:22,399 - trainer - INFO -     val_perplexity : 24.379741668701172
2025-01-16 14:10:22,399 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 14:10:22,399 - trainer - INFO - ================================================================================
2025-01-16 14:10:22,399 - trainer - INFO - Starting epoch 8 at 2025-01-16 14:10:22
2025-01-16 14:30:11,144 - trainer - INFO - [2025-01-16 14:30:11] Starting validation for epoch: 8
2025-01-16 14:30:26,252 - trainer - INFO - Epoch 8 completed at 2025-01-16 14:30:26
2025-01-16 14:30:26,253 - trainer - INFO -     epoch          : 8
2025-01-16 14:30:26,253 - trainer - INFO -     elapsed time   : 1203.8532509803772
2025-01-16 14:30:26,253 - trainer - INFO -     loss           : 106.65813433898622
2025-01-16 14:30:26,253 - trainer - INFO -     sim_loss       : 6.102680619513217
2025-01-16 14:30:26,253 - trainer - INFO -     gen_loss       : 100.55545368858832
2025-01-16 14:30:26,253 - trainer - INFO -     val_loss       : 185.23735930702904
2025-01-16 14:30:26,253 - trainer - INFO -     val_sim_loss   : 5.978736617348411
2025-01-16 14:30:26,253 - trainer - INFO -     val_gen_loss   : 179.2586217360063
2025-01-16 14:30:26,253 - trainer - INFO -     val_perplexity : 24.436260223388672
2025-01-16 14:30:26,253 - trainer - INFO -     val_accuracy   : 0.022727272727272728
2025-01-16 14:30:26,253 - trainer - INFO - ================================================================================
2025-01-16 14:30:26,253 - trainer - INFO - Starting epoch 9 at 2025-01-16 14:30:26
2025-01-16 14:50:14,826 - trainer - INFO - [2025-01-16 14:50:14] Starting validation for epoch: 9
2025-01-16 14:50:30,021 - trainer - INFO - Epoch 9 completed at 2025-01-16 14:50:30
2025-01-16 14:50:30,021 - trainer - INFO -     epoch          : 9
2025-01-16 14:50:30,021 - trainer - INFO -     elapsed time   : 1203.7672908306122
2025-01-16 14:50:30,021 - trainer - INFO -     loss           : 99.98996314053512
2025-01-16 14:50:30,021 - trainer - INFO -     sim_loss       : 5.9535590003666226
2025-01-16 14:50:30,021 - trainer - INFO -     gen_loss       : 94.03640416842788
2025-01-16 14:50:30,021 - trainer - INFO -     val_loss       : 188.18409451571378
2025-01-16 14:50:30,021 - trainer - INFO -     val_sim_loss   : 5.206021845340729
2025-01-16 14:50:30,021 - trainer - INFO -     val_gen_loss   : 182.97807173295453
2025-01-16 14:50:30,021 - trainer - INFO -     val_perplexity : 25.567655563354492
2025-01-16 14:50:30,021 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 14:50:30,021 - trainer - INFO - ================================================================================
2025-01-16 14:50:30,021 - trainer - INFO - Starting epoch 10 at 2025-01-16 14:50:30
2025-01-16 15:10:18,211 - trainer - INFO - [2025-01-16 15:10:18] Starting validation for epoch: 10
2025-01-16 15:10:33,299 - trainer - INFO - Epoch 10 completed at 2025-01-16 15:10:33
2025-01-16 15:10:33,300 - trainer - INFO -     epoch          : 10
2025-01-16 15:10:33,300 - trainer - INFO -     elapsed time   : 1203.2780039310455
2025-01-16 15:10:33,300 - trainer - INFO -     loss           : 94.94086457437544
2025-01-16 15:10:33,300 - trainer - INFO -     sim_loss       : 6.0018147804992115
2025-01-16 15:10:33,300 - trainer - INFO -     gen_loss       : 88.93904971601951
2025-01-16 15:10:33,300 - trainer - INFO -     val_loss       : 191.38612539117986
2025-01-16 15:10:33,300 - trainer - INFO -     val_sim_loss   : 3.824024872346358
2025-01-16 15:10:33,300 - trainer - INFO -     val_gen_loss   : 187.56209806962445
2025-01-16 15:10:33,300 - trainer - INFO -     val_perplexity : 24.872472763061523
2025-01-16 15:10:33,300 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 15:27:45,524 - train - INFO - TopicExpan(
  (doc_encoder): BertDocEncoder(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (phrase_decoder): TransformerPhraseDecoder(
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output_embeddings): Linear(in_features=768, out_features=30522, bias=False)
    (model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (topic_encoder): GCNTopicEncoder(
    (downward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (upward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (sideward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
  )
  (interaction): BilinearInteraction()
  (linear_combiner): Linear(in_features=1152, out_features=768, bias=True)
)
Trainable parameters: 142868480
2025-01-16 15:27:48,565 - trainer - INFO - Loading checkpoint: congress-save/models/checkpoint-epoch10.pth ...
2025-01-16 15:28:14,082 - trainer - INFO - Checkpoint loaded. Resume training from epoch 11
2025-01-16 15:28:14,085 - trainer - INFO - ================================================================================
2025-01-16 15:28:14,085 - trainer - INFO - Starting epoch 11 at 2025-01-16 15:28:14
2025-01-16 15:48:03,201 - trainer - INFO - [2025-01-16 15:48:03] Starting validation for epoch: 11
2025-01-16 15:48:18,660 - trainer - INFO - Epoch 11 completed at 2025-01-16 15:48:18
2025-01-16 15:48:18,660 - trainer - INFO -     epoch          : 11
2025-01-16 15:48:18,660 - trainer - INFO -     elapsed time   : 1204.5750958919525
2025-01-16 15:48:18,661 - trainer - INFO -     loss           : 88.05706505467049
2025-01-16 15:48:18,661 - trainer - INFO -     sim_loss       : 5.96681684534348
2025-01-16 15:48:18,661 - trainer - INFO -     gen_loss       : 82.0902481990074
2025-01-16 15:48:18,661 - trainer - INFO -     val_loss       : 195.6306656924161
2025-01-16 15:48:18,661 - trainer - INFO -     val_sim_loss   : 4.003547495061701
2025-01-16 15:48:18,661 - trainer - INFO -     val_gen_loss   : 191.62711871754038
2025-01-16 15:48:18,661 - trainer - INFO -     val_perplexity : 25.641103744506836
2025-01-16 15:48:18,661 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 15:48:18,661 - trainer - INFO - ================================================================================
2025-01-16 15:48:18,661 - trainer - INFO - Starting epoch 12 at 2025-01-16 15:48:18
2025-01-16 16:12:20,472 - train - INFO - TopicExpan(
  (doc_encoder): BertDocEncoder(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (phrase_decoder): TransformerPhraseDecoder(
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output_embeddings): Linear(in_features=768, out_features=30522, bias=False)
    (model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (topic_encoder): GCNTopicEncoder(
    (downward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (upward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (sideward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
  )
  (interaction): BilinearInteraction()
  (linear_combiner): Linear(in_features=1152, out_features=768, bias=True)
)
Trainable parameters: 142868480
2025-01-16 16:12:23,308 - trainer - INFO - Loading checkpoint: congress-save/models/checkpoint-epoch10.pth ...
2025-01-16 16:12:24,754 - trainer - INFO - Checkpoint loaded. Resume training from epoch 11
2025-01-16 16:12:24,757 - trainer - INFO - ================================================================================
2025-01-16 16:12:24,757 - trainer - INFO - Starting epoch 11 at 2025-01-16 16:12:24
2025-01-16 17:19:43,759 - train - INFO - TopicExpan(
  (doc_encoder): BertDocEncoder(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (phrase_decoder): TransformerPhraseDecoder(
    (input_embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (output_embeddings): Linear(in_features=768, out_features=30522, bias=False)
    (model): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (linear1): Linear(in_features=768, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=768, bias=True)
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (topic_encoder): GCNTopicEncoder(
    (downward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (upward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
    (sideward_layers): ModuleList(
      (0-1): 2 x GraphConv(in=384, out=384, normalization=right, activation=None)
    )
  )
  (interaction): BilinearInteraction()
  (linear_combiner): Linear(in_features=1152, out_features=768, bias=True)
)
Trainable parameters: 142868480
2025-01-16 17:19:46,200 - trainer - INFO - Loading checkpoint: congress-save/models/checkpoint-epoch10.pth ...
2025-01-16 17:20:21,826 - trainer - INFO - Checkpoint loaded. Resume training from epoch 11
2025-01-16 17:20:21,828 - trainer - INFO - ================================================================================
2025-01-16 17:20:21,828 - trainer - INFO - Starting epoch 11 at 2025-01-16 17:20:21
2025-01-16 17:28:55,828 - trainer - INFO - [2025-01-16 17:28:55] Starting validation for epoch: 11
2025-01-16 17:29:03,647 - trainer - INFO - Epoch 11 completed at 2025-01-16 17:29:03
2025-01-16 17:29:03,647 - trainer - INFO -     epoch          : 11
2025-01-16 17:29:03,647 - trainer - INFO -     elapsed time   : 521.8190848827362
2025-01-16 17:29:03,647 - trainer - INFO -     loss           : 90.27099151611328
2025-01-16 17:29:03,647 - trainer - INFO -     sim_loss       : 6.017202340933814
2025-01-16 17:29:03,647 - trainer - INFO -     gen_loss       : 84.25378920521902
2025-01-16 17:29:03,647 - trainer - INFO -     val_loss       : 196.32575971429998
2025-01-16 17:29:03,647 - trainer - INFO -     val_sim_loss   : 4.760126666589216
2025-01-16 17:29:03,647 - trainer - INFO -     val_gen_loss   : 191.56563065268776
2025-01-16 17:29:03,648 - trainer - INFO -     val_perplexity : 25.593185424804688
2025-01-16 17:29:03,648 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 17:29:03,648 - trainer - INFO - ================================================================================
2025-01-16 17:29:03,648 - trainer - INFO - Starting epoch 12 at 2025-01-16 17:29:03
2025-01-16 17:37:36,134 - trainer - INFO - [2025-01-16 17:37:36] Starting validation for epoch: 12
2025-01-16 17:37:43,955 - trainer - INFO - Epoch 12 completed at 2025-01-16 17:37:43
2025-01-16 17:37:43,955 - trainer - INFO -     epoch          : 12
2025-01-16 17:37:43,955 - trainer - INFO -     elapsed time   : 520.307511806488
2025-01-16 17:37:43,955 - trainer - INFO -     loss           : 87.76725228153057
2025-01-16 17:37:43,955 - trainer - INFO -     sim_loss       : 5.870830144714657
2025-01-16 17:37:43,956 - trainer - INFO -     gen_loss       : 81.89642217076239
2025-01-16 17:37:43,956 - trainer - INFO -     val_loss       : 198.72839979691938
2025-01-16 17:37:43,956 - trainer - INFO -     val_sim_loss   : 6.872992645610463
2025-01-16 17:37:43,956 - trainer - INFO -     val_gen_loss   : 191.85540702126244
2025-01-16 17:37:43,956 - trainer - INFO -     val_perplexity : 26.7264404296875
2025-01-16 17:37:43,956 - trainer - INFO -     val_accuracy   : 0.022727272727272728
2025-01-16 17:37:43,956 - trainer - INFO - ================================================================================
2025-01-16 17:37:43,956 - trainer - INFO - Starting epoch 13 at 2025-01-16 17:37:43
2025-01-16 17:46:16,307 - trainer - INFO - [2025-01-16 17:46:16] Starting validation for epoch: 13
2025-01-16 17:46:24,139 - trainer - INFO - Epoch 13 completed at 2025-01-16 17:46:24
2025-01-16 17:46:24,139 - trainer - INFO -     epoch          : 13
2025-01-16 17:46:24,139 - trainer - INFO -     elapsed time   : 520.183336019516
2025-01-16 17:46:24,139 - trainer - INFO -     loss           : 85.16684957096233
2025-01-16 17:46:24,139 - trainer - INFO -     sim_loss       : 5.665466595556013
2025-01-16 17:46:24,140 - trainer - INFO -     gen_loss       : 79.50138288659242
2025-01-16 17:46:24,140 - trainer - INFO -     val_loss       : 199.8920130296187
2025-01-16 17:46:24,140 - trainer - INFO -     val_sim_loss   : 5.616657582196322
2025-01-16 17:46:24,140 - trainer - INFO -     val_gen_loss   : 194.2753568129106
2025-01-16 17:46:24,140 - trainer - INFO -     val_perplexity : 25.756561279296875
2025-01-16 17:46:24,140 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 17:46:24,140 - trainer - INFO - ================================================================================
2025-01-16 17:46:24,140 - trainer - INFO - Starting epoch 14 at 2025-01-16 17:46:24
2025-01-16 17:54:56,632 - trainer - INFO - [2025-01-16 17:54:56] Starting validation for epoch: 14
2025-01-16 17:55:04,585 - trainer - INFO - Epoch 14 completed at 2025-01-16 17:55:04
2025-01-16 17:55:04,585 - trainer - INFO -     epoch          : 14
2025-01-16 17:55:04,586 - trainer - INFO -     elapsed time   : 520.4453930854797
2025-01-16 17:55:04,586 - trainer - INFO -     loss           : 83.255917229344
2025-01-16 17:55:04,586 - trainer - INFO -     sim_loss       : 5.959446858672138
2025-01-16 17:55:04,586 - trainer - INFO -     gen_loss       : 77.29647056617547
2025-01-16 17:55:04,586 - trainer - INFO -     val_loss       : 203.52778096632525
2025-01-16 17:55:04,586 - trainer - INFO -     val_sim_loss   : 6.4250077117573134
2025-01-16 17:55:04,586 - trainer - INFO -     val_gen_loss   : 197.1027737530795
2025-01-16 17:55:04,586 - trainer - INFO -     val_perplexity : 25.206085205078125
2025-01-16 17:55:04,586 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 17:55:04,586 - trainer - INFO - ================================================================================
2025-01-16 17:55:04,586 - trainer - INFO - Starting epoch 15 at 2025-01-16 17:55:04
2025-01-16 18:03:37,040 - trainer - INFO - [2025-01-16 18:03:37] Starting validation for epoch: 15
2025-01-16 18:03:44,842 - trainer - INFO - Epoch 15 completed at 2025-01-16 18:03:44
2025-01-16 18:03:44,842 - trainer - INFO -     epoch          : 15
2025-01-16 18:03:44,843 - trainer - INFO -     elapsed time   : 520.2563292980194
2025-01-16 18:03:44,843 - trainer - INFO -     loss           : 81.45157153381044
2025-01-16 18:03:44,843 - trainer - INFO -     sim_loss       : 5.734386645264887
2025-01-16 18:03:44,843 - trainer - INFO -     gen_loss       : 75.71718476328684
2025-01-16 18:03:44,843 - trainer - INFO -     val_loss       : 205.44769668579102
2025-01-16 18:03:44,843 - trainer - INFO -     val_sim_loss   : 5.605939529158852
2025-01-16 18:03:44,843 - trainer - INFO -     val_gen_loss   : 199.84175907481801
2025-01-16 18:03:44,843 - trainer - INFO -     val_perplexity : 28.91658592224121
2025-01-16 18:03:44,843 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 18:04:00,775 - trainer - INFO - Saving checkpoint: congress-save/models/checkpoint-epoch15.pth ...
2025-01-16 18:04:00,775 - trainer - INFO - ================================================================================
2025-01-16 18:04:00,775 - trainer - INFO - Starting epoch 16 at 2025-01-16 18:04:00
2025-01-16 18:12:33,228 - trainer - INFO - [2025-01-16 18:12:33] Starting validation for epoch: 16
2025-01-16 18:12:41,003 - trainer - INFO - Epoch 16 completed at 2025-01-16 18:12:41
2025-01-16 18:12:41,003 - trainer - INFO -     epoch          : 16
2025-01-16 18:12:41,003 - trainer - INFO -     elapsed time   : 520.2273671627045
2025-01-16 18:12:41,003 - trainer - INFO -     loss           : 80.16583030662727
2025-01-16 18:12:41,003 - trainer - INFO -     sim_loss       : 5.944519806393788
2025-01-16 18:12:41,003 - trainer - INFO -     gen_loss       : 74.22131040772395
2025-01-16 18:12:41,003 - trainer - INFO -     val_loss       : 204.29264406724408
2025-01-16 18:12:41,003 - trainer - INFO -     val_sim_loss   : 5.106175444342873
2025-01-16 18:12:41,003 - trainer - INFO -     val_gen_loss   : 199.18647063862193
2025-01-16 18:12:41,003 - trainer - INFO -     val_perplexity : 26.1213321685791
2025-01-16 18:12:41,003 - trainer - INFO -     val_accuracy   : 0.022727272727272728
2025-01-16 18:12:41,003 - trainer - INFO - ================================================================================
2025-01-16 18:12:41,003 - trainer - INFO - Starting epoch 17 at 2025-01-16 18:12:41
2025-01-16 18:21:13,424 - trainer - INFO - [2025-01-16 18:21:13] Starting validation for epoch: 17
2025-01-16 18:21:21,297 - trainer - INFO - Epoch 17 completed at 2025-01-16 18:21:21
2025-01-16 18:21:21,297 - trainer - INFO -     epoch          : 17
2025-01-16 18:21:21,297 - trainer - INFO -     elapsed time   : 520.2932252883911
2025-01-16 18:21:21,297 - trainer - INFO -     loss           : 79.07656840044467
2025-01-16 18:21:21,297 - trainer - INFO -     sim_loss       : 5.993274690092898
2025-01-16 18:21:21,297 - trainer - INFO -     gen_loss       : 73.08329372311113
2025-01-16 18:21:21,297 - trainer - INFO -     val_loss       : 207.0630874633789
2025-01-16 18:21:21,297 - trainer - INFO -     val_sim_loss   : 4.448069648309187
2025-01-16 18:21:21,297 - trainer - INFO -     val_gen_loss   : 202.61501867120916
2025-01-16 18:21:21,297 - trainer - INFO -     val_perplexity : 28.280405044555664
2025-01-16 18:21:21,297 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 18:21:21,297 - trainer - INFO - ================================================================================
2025-01-16 18:21:21,297 - trainer - INFO - Starting epoch 18 at 2025-01-16 18:21:21
2025-01-16 18:29:53,765 - trainer - INFO - [2025-01-16 18:29:53] Starting validation for epoch: 18
2025-01-16 18:30:01,655 - trainer - INFO - Epoch 18 completed at 2025-01-16 18:30:01
2025-01-16 18:30:01,655 - trainer - INFO -     epoch          : 18
2025-01-16 18:30:01,655 - trainer - INFO -     elapsed time   : 520.3576805591583
2025-01-16 18:30:01,655 - trainer - INFO -     loss           : 77.2570980185893
2025-01-16 18:30:01,655 - trainer - INFO -     sim_loss       : 5.619592948730535
2025-01-16 18:30:01,655 - trainer - INFO -     gen_loss       : 71.63750501983795
2025-01-16 18:30:01,655 - trainer - INFO -     val_loss       : 209.2994183627042
2025-01-16 18:30:01,655 - trainer - INFO -     val_sim_loss   : 4.204416903582486
2025-01-16 18:30:01,655 - trainer - INFO -     val_gen_loss   : 205.09499913995916
2025-01-16 18:30:01,656 - trainer - INFO -     val_perplexity : 28.070241928100586
2025-01-16 18:30:01,656 - trainer - INFO -     val_accuracy   : 0.022727272727272728
2025-01-16 18:30:01,656 - trainer - INFO - ================================================================================
2025-01-16 18:30:01,656 - trainer - INFO - Starting epoch 19 at 2025-01-16 18:30:01
2025-01-16 18:38:34,097 - trainer - INFO - [2025-01-16 18:38:34] Starting validation for epoch: 19
2025-01-16 18:38:41,966 - trainer - INFO - Epoch 19 completed at 2025-01-16 18:38:41
2025-01-16 18:38:41,966 - trainer - INFO -     epoch          : 19
2025-01-16 18:38:41,966 - trainer - INFO -     elapsed time   : 520.3100755214691
2025-01-16 18:38:41,966 - trainer - INFO -     loss           : 76.09494775231205
2025-01-16 18:38:41,966 - trainer - INFO -     sim_loss       : 5.479274542373952
2025-01-16 18:38:41,966 - trainer - INFO -     gen_loss       : 70.61567321132071
2025-01-16 18:38:41,966 - trainer - INFO -     val_loss       : 212.31273477727717
2025-01-16 18:38:41,966 - trainer - INFO -     val_sim_loss   : 4.2740921581333335
2025-01-16 18:38:41,966 - trainer - INFO -     val_gen_loss   : 208.0386425365101
2025-01-16 18:38:41,966 - trainer - INFO -     val_perplexity : 29.065690994262695
2025-01-16 18:38:41,966 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 18:38:41,966 - trainer - INFO - ================================================================================
2025-01-16 18:38:41,966 - trainer - INFO - Starting epoch 20 at 2025-01-16 18:38:41
2025-01-16 18:47:14,439 - trainer - INFO - [2025-01-16 18:47:14] Starting validation for epoch: 20
2025-01-16 18:47:22,119 - trainer - INFO - Epoch 20 completed at 2025-01-16 18:47:22
2025-01-16 18:47:22,119 - trainer - INFO -     epoch          : 20
2025-01-16 18:47:22,120 - trainer - INFO -     elapsed time   : 520.1528322696686
2025-01-16 18:47:22,120 - trainer - INFO -     loss           : 75.34448213150252
2025-01-16 18:47:22,120 - trainer - INFO -     sim_loss       : 5.76120616743962
2025-01-16 18:47:22,120 - trainer - INFO -     gen_loss       : 69.58327589082481
2025-01-16 18:47:22,120 - trainer - INFO -     val_loss       : 216.8272223039107
2025-01-16 18:47:22,120 - trainer - INFO -     val_sim_loss   : 5.27118689363653
2025-01-16 18:47:22,120 - trainer - INFO -     val_gen_loss   : 211.55603616887873
2025-01-16 18:47:22,120 - trainer - INFO -     val_perplexity : 28.287817001342773
2025-01-16 18:47:22,120 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 18:47:38,047 - trainer - INFO - Saving checkpoint: congress-save/models/checkpoint-epoch20.pth ...
2025-01-16 18:47:38,048 - trainer - INFO - ================================================================================
2025-01-16 18:47:38,048 - trainer - INFO - Starting epoch 21 at 2025-01-16 18:47:38
2025-01-16 18:56:10,445 - trainer - INFO - [2025-01-16 18:56:10] Starting validation for epoch: 21
2025-01-16 18:56:18,316 - trainer - INFO - Epoch 21 completed at 2025-01-16 18:56:18
2025-01-16 18:56:18,316 - trainer - INFO -     epoch          : 21
2025-01-16 18:56:18,316 - trainer - INFO -     elapsed time   : 520.2676484584808
2025-01-16 18:56:18,316 - trainer - INFO -     loss           : 74.42472716943541
2025-01-16 18:56:18,316 - trainer - INFO -     sim_loss       : 5.623377769103097
2025-01-16 18:56:18,316 - trainer - INFO -     gen_loss       : 68.80134937039655
2025-01-16 18:56:18,316 - trainer - INFO -     val_loss       : 215.40357294949618
2025-01-16 18:56:18,316 - trainer - INFO -     val_sim_loss   : 3.9291233257813887
2025-01-16 18:56:18,316 - trainer - INFO -     val_gen_loss   : 211.47445141185415
2025-01-16 18:56:18,316 - trainer - INFO -     val_perplexity : 27.928747177124023
2025-01-16 18:56:18,316 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 18:56:18,316 - trainer - INFO - ================================================================================
2025-01-16 18:56:18,316 - trainer - INFO - Starting epoch 22 at 2025-01-16 18:56:18
2025-01-16 19:04:50,825 - trainer - INFO - [2025-01-16 19:04:50] Starting validation for epoch: 22
2025-01-16 19:04:58,584 - trainer - INFO - Epoch 22 completed at 2025-01-16 19:04:58
2025-01-16 19:04:58,584 - trainer - INFO -     epoch          : 22
2025-01-16 19:04:58,584 - trainer - INFO -     elapsed time   : 520.2672455310822
2025-01-16 19:04:58,584 - trainer - INFO -     loss           : 73.60262879328941
2025-01-16 19:04:58,584 - trainer - INFO -     sim_loss       : 5.85160853943199
2025-01-16 19:04:58,584 - trainer - INFO -     gen_loss       : 67.75102019997972
2025-01-16 19:04:58,584 - trainer - INFO -     val_loss       : 219.1362665349787
2025-01-16 19:04:58,584 - trainer - INFO -     val_sim_loss   : 4.858742757277056
2025-01-16 19:04:58,584 - trainer - INFO -     val_gen_loss   : 214.2775226939808
2025-01-16 19:04:58,584 - trainer - INFO -     val_perplexity : 28.709440231323242
2025-01-16 19:04:58,584 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 19:04:58,584 - trainer - INFO - ================================================================================
2025-01-16 19:04:58,584 - trainer - INFO - Starting epoch 23 at 2025-01-16 19:04:58
2025-01-16 19:13:31,066 - trainer - INFO - [2025-01-16 19:13:31] Starting validation for epoch: 23
2025-01-16 19:13:38,786 - trainer - INFO - Epoch 23 completed at 2025-01-16 19:13:38
2025-01-16 19:13:38,786 - trainer - INFO -     epoch          : 23
2025-01-16 19:13:38,786 - trainer - INFO -     elapsed time   : 520.2016217708588
2025-01-16 19:13:38,786 - trainer - INFO -     loss           : 72.60749718205845
2025-01-16 19:13:38,786 - trainer - INFO -     sim_loss       : 5.782512390509766
2025-01-16 19:13:38,786 - trainer - INFO -     gen_loss       : 66.82498479245314
2025-01-16 19:13:38,786 - trainer - INFO -     val_loss       : 217.82571099021217
2025-01-16 19:13:38,786 - trainer - INFO -     val_sim_loss   : 5.307870225472883
2025-01-16 19:13:38,786 - trainer - INFO -     val_gen_loss   : 212.51783995194867
2025-01-16 19:13:38,786 - trainer - INFO -     val_perplexity : 30.66954231262207
2025-01-16 19:13:38,786 - trainer - INFO -     val_accuracy   : 0.03409090909090909
2025-01-16 19:13:38,786 - trainer - INFO - ================================================================================
2025-01-16 19:13:38,786 - trainer - INFO - Starting epoch 24 at 2025-01-16 19:13:38
2025-01-16 19:22:11,273 - trainer - INFO - [2025-01-16 19:22:11] Starting validation for epoch: 24
2025-01-16 19:22:19,096 - trainer - INFO - Epoch 24 completed at 2025-01-16 19:22:19
2025-01-16 19:22:19,097 - trainer - INFO -     epoch          : 24
2025-01-16 19:22:19,097 - trainer - INFO -     elapsed time   : 520.3100674152374
2025-01-16 19:22:19,097 - trainer - INFO -     loss           : 71.32365385596432
2025-01-16 19:22:19,097 - trainer - INFO -     sim_loss       : 5.65477119270916
2025-01-16 19:22:19,097 - trainer - INFO -     gen_loss       : 65.66888257875965
2025-01-16 19:22:19,097 - trainer - INFO -     val_loss       : 219.96140410683373
2025-01-16 19:22:19,097 - trainer - INFO -     val_sim_loss   : 4.293484500863335
2025-01-16 19:22:19,097 - trainer - INFO -     val_gen_loss   : 215.66791933233088
2025-01-16 19:22:19,097 - trainer - INFO -     val_perplexity : 28.759159088134766
2025-01-16 19:22:19,097 - trainer - INFO -     val_accuracy   : 0.0
2025-01-16 19:22:19,097 - trainer - INFO - ================================================================================
2025-01-16 19:22:19,097 - trainer - INFO - Starting epoch 25 at 2025-01-16 19:22:19
2025-01-16 19:30:51,556 - trainer - INFO - [2025-01-16 19:30:51] Starting validation for epoch: 25
2025-01-16 19:30:59,330 - trainer - INFO - Epoch 25 completed at 2025-01-16 19:30:59
2025-01-16 19:30:59,330 - trainer - INFO -     epoch          : 25
2025-01-16 19:30:59,330 - trainer - INFO -     elapsed time   : 520.232764005661
2025-01-16 19:30:59,330 - trainer - INFO -     loss           : 70.49671587398396
2025-01-16 19:30:59,330 - trainer - INFO -     sim_loss       : 5.732593714152996
2025-01-16 19:30:59,330 - trainer - INFO -     gen_loss       : 64.76412216679967
2025-01-16 19:30:59,330 - trainer - INFO -     val_loss       : 218.7717250477184
2025-01-16 19:30:59,330 - trainer - INFO -     val_sim_loss   : 4.401597543196245
2025-01-16 19:30:59,330 - trainer - INFO -     val_gen_loss   : 214.37012897838247
2025-01-16 19:30:59,330 - trainer - INFO -     val_perplexity : 29.98039436340332
2025-01-16 19:30:59,330 - trainer - INFO -     val_accuracy   : 0.011363636363636364
2025-01-16 19:31:15,496 - trainer - INFO - Saving checkpoint: congress-save/models/checkpoint-epoch25.pth ...
2025-01-16 19:31:15,496 - trainer - INFO - ================================================================================
2025-01-16 19:31:15,497 - trainer - INFO - Starting epoch 26 at 2025-01-16 19:31:15
2025-01-16 19:39:47,993 - trainer - INFO - [2025-01-16 19:39:47] Starting validation for epoch: 26
2025-01-16 19:39:55,815 - trainer - INFO - Epoch 26 completed at 2025-01-16 19:39:55
2025-01-16 19:39:55,816 - trainer - INFO -     epoch          : 26
2025-01-16 19:39:55,816 - trainer - INFO -     elapsed time   : 520.3187708854675
2025-01-16 19:39:55,816 - trainer - INFO -     loss           : 69.64095596199606
2025-01-16 19:39:55,816 - trainer - INFO -     sim_loss       : 5.720395561719118
2025-01-16 19:39:55,816 - trainer - INFO -     gen_loss       : 63.92056032816569
2025-01-16 19:39:55,816 - trainer - INFO -     val_loss       : 225.5306751944802
2025-01-16 19:39:55,816 - trainer - INFO -     val_sim_loss   : 5.486561168323863
2025-01-16 19:39:55,816 - trainer - INFO -     val_gen_loss   : 220.04411818764427
2025-01-16 19:39:55,816 - trainer - INFO -     val_perplexity : 29.415475845336914
2025-01-16 19:39:55,816 - trainer - INFO -     val_accuracy   : 0.022727272727272728
2025-01-16 19:39:55,816 - trainer - INFO - ================================================================================
2025-01-16 19:39:55,816 - trainer - INFO - Starting epoch 27 at 2025-01-16 19:39:55
